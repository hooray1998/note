# 爬虫学习

## Requests库的基础方法如下

方法|说明|对应HTTP的
:-|:-|:-:
request()|构造一个请求，支撑以下各方法的基础方法|
get()|获取HTML网页|GET
head()|获取网页头信息|HEAD
post()|向HTML网页提交POST请求的方法|POST
put()|向HTML网页提交PUT请求的方法|PUT
patch()|向HTML网页提交局部修改请求|PATCH
delete()|向HTML网页提交删除请求|DELTE

## Request帮助文档
* [request快速上手](http://docs.python-requests.org/zh_CN/latest/user/quickstart.html)



### requests 发送网络请求
```python
import requests
r = requests.get("https://apt.github.com/events")
r = requests.post('http://httpbin.org/post', data = {'key':'value'})
r = requests.put('http://httpbin.org/put', data = {'key':'value'})
r = requests.delete('http://httpbin.org/delete')
r = requests.head('http://httpbin.org/get')
r = requests.options('http://httpbin.org/get')

```

### 传递URL参数
```python
import requests
payload = {'key1': 'value1', 'key2': 'value2'}
r = requests.get("http://httpbin.org/get", params=payload)
print(r.url)
#[output]:http://httpbin.org/get?key1=value1&key2=value2
payload = {'key1': 'value1', 'key2': ['value2', 'value3']}
r = requests.get('http://httpbin.org/get', params=payload)
print(r.url)
#[output]:http://httpbin.org/get?key1=value1&key2=value2&key2=value3
```

### 响应内容
```python
import requests
r = requests.get("https://www.baidu.com")
r.encoding = 'utf-8'
print(r.encoding)
```

#### 二进制响应内容
r.content

#### JSON响应内容
r.json() # 解析错误会抛出异常，不抛出异常也未必成功，可检查状态码


#### 原始响应内容

要设置stream=True
```python
>>> r = requests.get('https://api.github.com/events', stream=True)
>>> r.raw
<requests.packages.urllib3.response.HTTPResponse object at 0x101194810>
>>> r.raw.read(10)
'\x1f\x8b\x08\x00\x00\x00\x00\x00\x00\x03'


# 以下面的模式将文本流保存到文件：
with open(filename, 'wb') as fd:
    for chunk in r.iter_content(chunk_size):
        fd.write(chunk)
```

### 定制请求头

传递dict给headers

```python
>>> url = 'https://api.github.com/some/endpoint'
>>> headers = {'user-agent': 'my-app/0.0.1'}

>>> r = requests.get(url, headers=headers)
```
有些情况header会无效
1. 如果在 .netrc 中设置了用户认证信息，使用 headers= 设置的授权就不会生效。而如果设置了 auth= 参数，``.netrc`` 的设置就无效了。
2. 如果被重定向到别的主机，授权 header 就会被删除。
3. 代理授权 header 会被 URL 中提供的代理身份覆盖掉。
4. 在我们能判断内容长度的情况下，header 的 Content-Length 会被改写。


### 响应状态码

```python
r.status_code
# Requests还附带了一个内置的状态码查询对象，一般为200
>>> r.status_code == requests.codes.ok
True

# 通过Response.raise_for_status() 抛出异常, 如果正常，则无输出
>>> r.raise_for_status()
Traceback (most recent call last):
  File "requests/models.py", line 832, in raise_for_status
    raise http_error
requests.exceptions.HTTPError: 404 Client Error

```

### 响应头
```python
>>> r.headers # 返回字典
{
    'content-encoding': 'gzip',
    'transfer-encoding': 'chunked',
    'connection': 'close',
    'server': 'nginx/1.0.4',
    'x-runtime': '148ms',
    'etag': '"e1ca502697e5c9317743dc078f67693f"',
    'content-type': 'application/json'
}

# 查看特定的值
>>> r.headers['Content-Type']
'application/json'

>>> r.headers.get('content-type')
'application/json'
```

### Cookie

Cookie是由服务器端生成，发送给User-Agent（一般是浏览器），浏览器会将Cookie的key/value保存到某个目录下的文本文件内，下次请求同一网站时就发送该Cookie给服务器（前提是浏览器设置为启用cookie）。Cookie名称和值可以由服务器端开发自己定义，对于JSP而言也可以直接写入jsessionid，这样服务器可以知道该用户是否是合法用户以及是否需要重新登录等，服务器可以设置或读取Cookies中包含信息，借此维护用户跟服务器会话中的状态。

Cookie信息则存放在HTTP请求头（Request Header）了。有了Cookie这样的技术实现，服务器在接收到来自客户端浏览器的请求之后，就能够通过分析存放于请求头的Cookie得到客户端特有的信息，从而动态生成与该客户端相对应的内容。通常，我们可以从很多网站的登录界面中看到“请记住我”这样的选项，如果你勾选了它之后再登录，那么在下一次访问该网站的时候就不需要进行重复而繁琐的登录动作了，而这个功能就是通过Cookie实现的。

如果某个响应中包含一些 cookie，你可以快速访问它们：
```python
>>> url = 'http://example.com/some/cookie/setting/url'
>>> r = requests.get(url)

>>> r.cookies['example_cookie_name']
'example_cookie_value'
```
要想发送你的cookies到服务器，可以使用 cookies 参数：
```python
>>> url = 'http://httpbin.org/cookies'
>>> cookies = dict(cookies_are='working')

>>> r = requests.get(url, cookies=cookies)
>>> r.text
'{"cookies": {"cookies_are": "working"}}'

```
Cookie 的返回对象为 RequestsCookieJar，它的行为和字典类似，但接口更为完整，适合跨域名跨路径使用。你还可以把 Cookie Jar 传到 Requests 中：

```python
>>> jar = requests.cookies.RequestsCookieJar()
>>> jar.set('tasty_cookie', 'yum', domain='httpbin.org', path='/cookies')
>>> jar.set('gross_cookie', 'blech', domain='httpbin.org', path='/elsewhere')
>>> url = 'http://httpbin.org/cookies'
>>> r = requests.get(url, cookies=jar)
>>> r.text
'{"cookies": {"tasty_cookie": "yum"}}'
```

### 重定向与请求历史
默认情况下，除了 HEAD, Requests 会自动处理其他所有重定向。
可以使用响应对象的history方法来追踪重定向，返回列表，从老到近
eg. Github会把所有的http请求重定向到https
```python
>>> r = requests.get('http://github.com')
>>> r.url # ~~~~~~~~~~~~~~~已变成https
'https://github.com/'
>>> r.status_code
200
>>> r.history # size不为0，有重定向
[<Response [301]>]
```
如果你使用的是GET、OPTIONS、POST、PUT、PATCH 或者 DELETE，那么你可以通过 allow_redirects 参数禁用重定向处理：
```python
>>> r = requests.get('http://github.com', allow_redirects=False)
>>> r.status_code
301
>>> r.history # size为0，无重定向
[]
```
如果你使用了 HEAD，你也可以启用重定向：
```python
>>> r = requests.head('http://github.com', allow_redirects=True)
>>> r.url
'https://github.com/'
>>> r.history # 启动了重定向
[<Response [301]>]
```

### 超时
```python
>>> requests.get('http://github.com', timeout=0.001)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
requests.exceptions.Timeout: HTTPConnectionPool(host='github.com', port=80): Request timed out. (timeout=0.001)
```
timeout 并不是整个下载响应的时间限制，而是如果服务器在 timeout 秒内没有应答，将会引发一个异常（更精确地说，是在 timeout 秒内没有从基础套接字上接收到任何字节的数据时）
如果不使用，你的程序可能会永远失去响应：

### 错误与异常

1. 遇到网络问题（如：DNS 查询失败、拒绝连接等）时，Requests 会抛出一个 ConnectionError 异常。
2. 如果 HTTP 请求返回了不成功的状态码， Response.raise_for_status() 会抛出一个 HTTPError 异常。
3. 若请求超时，则抛出一个 Timeout 异常。
4. 若请求超过了设定的最大重定向次数，则会抛出一个 TooManyRedirects 异常。
5. 所有Requests显式抛出的异常都继承自 requests.exceptions.RequestException 。
