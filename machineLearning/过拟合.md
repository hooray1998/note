# 过拟合

现象：
训练集准确率很高，而测试集却很低


## 解决办法

1. 提前终止策略（hold out）(从训练过程的角度)
	- 将数据集分为 训练集|验证集|测试集
	- 当验证集检测到准确率饱和时停止
2. 增加训练数据(从数据的角度)
	- 增加新的数据
	- 人为扩展数据(eg. 手写图片左右旋转15°，语音加噪声|加速|减速)
3. 降低网络的规模(从模型的角度)
	- 神经网络的潜力会降低
	- **丢弃一些不能帮助我们正确预测的特征**。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如**PCA**）
4. 正则化，规范化
	- 对w增加惩罚，不对b操作
	1. L2通式: 
		- C = Co + (λ/2n)∑w²
		- 梯度下降: w = (1 - λ/n)w - η(∂Co)/∂w
		- 同过一个小于1的比例向0收缩
	2. L1通式:
		- C = Co + (λ/2n)∑|w|
		- 梯度下降: w = w - (λ/n)sgn(w) - η(∂Co)/∂w
		- 同过一个固定的常量向0收缩
	- 当w很大时，L2速度快，反之，L1更快
5. Dropout
	- 随机丢弃一定比例的隐藏的神经元
	- 可以训练神经网络的健壮性
	- Dropout不同的神经元，就相当于训练了多个网络



## 过拟合现象

![img](https://github.com/hooray1998/Coursera-ML-AndrewNg-Notes/raw/master/images/72f84165fbf1753cd516e65d5e91c0d3.jpg)

第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。

![](https://github.com/hooray1998/Coursera-ML-AndrewNg-Notes/raw/master/images/be39b497588499d671942cc15026e4a2.jpg)

分类问题中也存在这样的问题：

就以多项式理解，的次数越高，拟合的越好，但相应的预测的能力就可能变差。

### 解决办法

1. **丢弃一些不能帮助我们正确预测的特征**。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如**PCA**）
2. **正则化**。 保留所有的特征，但是减少参数的大小（**magnitude**）。

## 代价函数

像上面的模型$${h_\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}+{\theta_{3}}{x_{3}^3}+{\theta_{4}}{x_{4}^4}$$

我们知道是由于高次项的影响导致过拟合，所以可以针对性的设置一些惩罚

但如果有很多特征，我们不知道哪些影响的，那就都添加惩罚，让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设


其中**入**又称为**正则化参数**（**Regularization Parameter**）。 注：根据惯例，我们不对thera0进行惩罚。  

对比结果如下：

![](https://github.com/hooray1998/Coursera-ML-AndrewNg-Notes/raw/master/images/ea76cc5394cf298f2414f230bcded0bd.jpg)

但如果正则化参数过大，会导致拟合结果如图中红线所示，欠拟合。

所以要选择一个合适的

## 正则化线性回归

 如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对进行正则化，所以梯度下降算法将分两种情形：

$${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{0}^{(i)}})$$

$${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}+\frac{\lambda }{m}{\theta_j}]$$

$$for\ j=1,2,...n$$

对上面的算法中$$ j=1,2,...,n$$ 时的更新式子进行调整可得：

$${\theta_j}:={\theta_j}(1-a\frac{\lambda }{m})-a\frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}​$$ 可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令$$\theta $$值减少了一个额外的值。

我们同样也可以利用正规方程来求解正则化线性回归模型，方法如下所示：

![](https://github.com/hooray1998/Coursera-ML-AndrewNg-Notes/raw/master/images/71d723ddb5863c943fcd4e6951114ee3.png)

图中的矩阵尺寸为 $$(n+1)*(n+1)$$。

## 正则化的逻辑回归模型

$$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}+\frac{\lambda }{2m}\sum\limits_{j=1}^{n}{\theta _{j}^{2}}$$

Python代码

```python
import numpy as np

def costReg(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X*theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X*theta.T)))
    reg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:theta.shape[1]],2))
    return np.sum(first - second) / (len(X)) + reg
```

梯度下降算法为：

$$Repeat\ until\ convergence$${

$${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{0}^{(i)}})$$
 $${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}+\frac{\lambda }{m}{\theta_j}]$$
 $$for\ j=1,2,...n$$
 }

1. 虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但由于两者的$${h_\theta}\left( x \right)$$不同所以还是有很大差别。

2. $${\theta_{0}}$$不参与其中的任何一个正则化
