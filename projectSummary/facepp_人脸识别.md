## 人脸识别

### 人脸检测
	- 使用opencv自带的库函数进行检测，主要算法有adaboost，Harr特征，LBP算法
- 构建简易的卷积神经网络模型
	- 基于keras深度学习框架
	- 根据一个简单的神经网络示例入手，建立了更合适的神经网络
		- Model和Sequential API很强大
		- 就能用Keras创建简单或很复杂的神经网络

- 通过分析人脸特征构建最终的网络结构
	- 由于人脸图片较小，纹理密度大，故采用多层卷积提取更多特征
	- 由于图片信息密度大，池化采用较小池化层
	- 由于种类多，采用较多神经元
	- 卷积-(激活函数)-> 池化 -> 卷积 -(激活)-> 池化 -> 全连接层 -(激活)-> 输出层

- 测试优化模型,调整参数
	- 池化规模
		- 刚开始无用的信息多一点，越到后面有用的信息越多
		- 所以先大后小比先小后大效果好，但都不能太大，因为图片较小，信息密度大
	- 卷积核
		- 缩小卷积核，提取更细节的特征
	- 激励函数
		- tanh激励函数在浅层网络中不会出现梯度消失问题，效果良好
		- 激励函数的作用越往后体现的越小,在这里三者的区别主要体现在x小于0的取值上
	- Dropout率
		- dropout1低一点好
		- dense(全连接层)小一点或者非常大都有好处
		- Dropout2适当的高一些好
	- 分析特征
		- shuffle
		- batch_size(批量训练的数目)
			- 样本随机

    model = Sequential()  #(连续的)
		卷积Conv2D(5, kernel_size=choice_kernel_size, input_shape=(Weidu, img_rows, img_cols))
			激励Activation(choice_Activation_1) #sigmod,relu,tanh
				池化MaxPooling2D(pool_size=choice_Pooling_size_1)
					卷积Conv2D(10, kernel_size=choice_kernel_size)
						激励Activation(choice_Activation_2)
							池化MaxPooling2D(pool_size=choice_Pooling_size_2)
								Dropout(number_Dropout_1)
									拉直Flatten()
										全连接层Dense(number_Dense)) #Full connection
											激励Activation(choice_Activation_2)
												Dropout(number_Dropout_2)
													类别数量Dense(nb_classes)
														选取最大Activation('softmax')

ANN 人工神经网络
CNN 卷积神经网络

## 学习神经网络的知识
### first
	- 最简单的非线性问题，异或xor =( nand )and( or )
	- 神经网络的初值如何选?
	- 选0，因为激励函数0处导数大，收敛快
	- BP算法框架有什么问题？
		- 采用的是梯度下降的方式，可能收敛于局部最小值
		- 解决方法：
		- 多选一些起始点，以期达到全局最小值
	- BP遇到平坦区域
		- 加冲量解决
	- 自适应的学习率
		- 太小,收敛慢，容易受噪声影响
		- 太大,收敛快，可能局部震荡
	- ANN人工神经网络什么时候有效
		1. 样本可以被一组具有实数值的特征向量表示。
		1. 目标输出可以是离散的值，连续的实数值或者
		1. 一个向量。
		1. 训练样本可能包含错误。
		1. 能够接受较长时间的训练：几秒、几分钟、几
		1. 小时、几天。
		1. 对于学到的函数需要快速评估。
		1. 对于学到的函数的解释并不重要。
		1. o 权重很难被解释
	- ANN人工神经网络的求解采用的主要流行方法是误差逆传播（Back propagation，BP）算法。
### second
	- 深度神经网络的三个步骤
		1. 定义一组函数
		2. 给定评价标准
		3. 挑出最好的函数
	- 越深参数越多，性能通常越好
	- 深度-> 模块化(解决了数据少的问题)
		- 先分出男女和长短发， 然后决定是长发男还是短发女
		- 每层的分类器都是由上一层组成
	- 模块化的优势
		- 非常相似的输入，完全不同的输出
		- 完全不同的输入，非常相似的输出
	- 深度越深有可能使效果越来越差
		- 原因：梯度消失
		- 方法：选用新的激励函数ReLU
			- 好处：计算快，仿生效果
	- 误差正则化：L1，L2
	- Dropout(类比绑沙袋训练，相当于训练了多个子树，最后求平均)
		- 训练时随机丢掉%p的神经元
		- 然后用剩下的训练
		- 测试时No Dropout，所有的权重要乘以(1-%p)
		- 使用场景：训练结果很好，测试结果不那么好的时候
	- 图像语音处理使用CNN的原因
		1. 重要的模式（鸟嘴检测）通常都比图片本身小
			- 用局部就可以发现某个模式，并且局部参数很少
		2. 相同的模式有可能在不同的区域
		3. 二次抽样不会改变我的关注的模式
	- 卷积网络
		- 卷积Convolution -> Max Pooling -> 卷积Convolution -> Max Pooling ... -> Flatten
		- 卷积
			- 卷积通过一定大小的卷积核与图片卷积，有几个卷积核就会生成几个通道
			- 每个卷积核对应位置的权值共享
		- 池化
			- 新图比原图小，新图的通道数等于卷积核的个数
		- 拉直
			- 将参数变成一维
### 循环神经网络
	- 如何用向量表达词
		- one-hot编码
			- 英文中可以使用单词进行哈希
	- 例子
		- 我将在3月15号离开北京
		- 我将在3月15号达到北京
		- 输入北京
			- 如何第一次输出出发地，第二次输出目的地
		- 需要神经网络具有记忆功能
	- 循环神经网络隐含层的输出存储在记忆单元里
	- 可以将记忆单元看成额外的输入
		- 效果：
		- 两次连续输入【1,1】得到不同的输出
		- 输入顺序也影响结果

